require("dotenv").config();

const getTimeseries = require("./getTimeseries");
const buildForecastPrompt = require("./buildForecastPrompt");
const saveForecast = require("./saveForecast");

// üî• JSON EXTRACTOR ‚Äî b√°rmit √≠r az AI, ebb≈ël JSON lesz
function extractJson(text) {
  if (!text) throw new Error("Empty AI response");

  const start = text.indexOf("[");
  const end = text.lastIndexOf("]");

  if (start === -1 || end === -1) {
    throw new Error("No JSON array found in AI output");
  }

  const jsonString = text.slice(start, end + 1);

  try {
    return JSON.parse(jsonString);
  } catch (err) {
    console.error("‚ùå JSON parse error on extracted string:", jsonString);
    throw err;
  }
}

// üî• OLLAMA WRAPPER
async function callOllama(prompt) {
  const res = await fetch("http://127.0.0.1:11434/api/generate", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      model: "llama3:latest",
      prompt,
      stream: false,
      keep_alive: 0,
      options: { num_predict: 400 }
    }),
  });

  const data = await res.json();
  return data.response;
}

async function runForecastPipeline() {
  console.log("üîç √ìr√°s adatok lek√©r√©se...");
  const timeseries = await getTimeseries(24 * 7);

  for (const category of Object.keys(timeseries)) {
    console.log(`\nüìä Kateg√≥ria: ${category}`);

    const points = timeseries[category];
    const prompt = buildForecastPrompt(category, points);

    console.log("ü§ñ AI el≈ërejelz√©s gener√°l√°sa...");
    const raw = await callOllama(prompt);

    let forecast;
    try {
      forecast = extractJson(raw);
    } catch (err) {
      console.error("‚ùå JSON extract/parse error:", err);
      continue; // megy tov√°bb a k√∂vetkez≈ë kateg√≥ri√°ra
    }

    console.log("üíæ Ment√©s DB-be...");
    await saveForecast(category, forecast);

    console.log("‚úî K√©sz!");
  }

  console.log("\nüéâ Minden kateg√≥ria el≈ërejelz√©se elk√©sz√ºlt!");
}

runForecastPipeline().catch(console.error);
