require("dotenv").config();

const getTimeseries = require("./getTimeseries");
const buildForecastPrompt = require("./buildForecastPrompt");
const saveForecast = require("./saveForecast");

// Ollama wrapper
async function callOllama(prompt) {
  const res = await fetch("http://127.0.0.1:11434/api/generate", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      model: "llama3:latest",
      prompt,
      stream: false,
      keep_alive: 0,
      options: { num_predict: 400 }
    }),
  });

  const data = await res.json();
  return data.response;
}

async function runForecastPipeline() {
  console.log("üîç √ìr√°s adatok lek√©r√©se...");
  const timeseries = await getTimeseries(24 * 7);

  for (const category of Object.keys(timeseries)) {
    console.log(`\nüìä Kateg√≥ria: ${category}`);

    const points = timeseries[category];
    const prompt = buildForecastPrompt(category, points);

    console.log("ü§ñ AI el≈ërejelz√©s gener√°l√°sa...");
    const raw = await callOllama(prompt);

    let forecast;
    try {
      forecast = JSON.parse(raw);
    } catch {
      console.error("‚ùå JSON parse error");
      continue;
    }

    console.log("üíæ Ment√©s DB-be...");
    await saveForecast(category, forecast);

    console.log("‚úî K√©sz!");
  }

  console.log("\nüéâ Minden kateg√≥ria el≈ërejelz√©se elk√©sz√ºlt!");
}

runForecastPipeline().catch(console.error);
